{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Input, Merge, Convolution1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CORPORA_PATH = \"/home/mgimenez/Dev/corpora/SemEval/SemEval_2017/2017_English_final/2017_English_final/GOLD/Subtask_A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_FILE = join(CORPORA_PATH, \"twitter-2016train-A.txt\")\n",
    "TEST_FILE = join(CORPORA_PATH, \"twitter-2016test-A.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(text):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`#@]\", \" \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \\'s\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" \\'ve\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" n\\'t\", text)\n",
    "    text = re.sub(r\"\\'re\", \" \\'re\", text)\n",
    "    text = re.sub(r\"\\'d\", \" \\'d\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" \\'ll\", text)\n",
    "    text = re.sub(r\",\", \" , \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\(\", \" \\( \", text)\n",
    "    text = re.sub(r\"\\)\", \" \\) \", text)\n",
    "    text = re.sub(r\"\\?\", \" \\? \", text)\n",
    "    text = re.sub(r\"c\\'\", \" c\\' \", text)\n",
    "    # Delete more than 2 spaces\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    # Delete every character that appear more than 3 times\n",
    "    text = re.sub(r'(.)\\1{3,}', r'\\1\\1\\1', text)\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_file(filepath, tokenize=False, labeled=True):\n",
    "    \"\"\" \n",
    "    Load a dataset file, split it in two lists: tweets and their tags; \n",
    "    without repeatead instances. \n",
    "    Also, if the flag tokenize is true, tokenize and clean each tweet. \n",
    "    \"\"\"\n",
    "    # Dictionary to encode the output variable\n",
    "    labels_tags = {'neutral':[0, 1, 0], 'negative':[1, 0, 0], 'positive':[0, 0, 1]}\n",
    "    keys, tweets, labels = set(), [], []\n",
    "    with open(filepath) as dataset_file:\n",
    "        for line in dataset_file:\n",
    "            # TODO: This IMPLIES THERE IS AN ERROR IN THE INPUT FILE \n",
    "            if len(line.strip().split('\\t')) != 3:\n",
    "                # print(\"--->\", len(line.strip().split('\\t')), line, line.strip().split('\\t'))\n",
    "                # print(\"--->\", len(line.strip().split('\\t')))\n",
    "                continue\n",
    "                    \n",
    "            # If this file is labeled because it's train file save the labels.\n",
    "            if labeled:\n",
    "                key, sentiment, tweet = line.strip().split('\\t')[:3]\n",
    "            else:\n",
    "                key, tweet = line.strip().split('\\t')\n",
    "                \n",
    "            # Save the tweets that are not repeated\n",
    "            if key not in keys:\n",
    "                if tokenize:\n",
    "                    tweet = clean_str(tweet)\n",
    "                tweets.append(tweet.split(\" \"))\n",
    "                if labeled:\n",
    "                    labels.append(labels_tags[sentiment])\n",
    "                keys.add(key)\n",
    "    \n",
    "    # TODO: Move this to the unittest\n",
    "    if labeled:\n",
    "        assert len(tweets) == len(labels)\n",
    "    assert len(keys) ==  len(tweets)\n",
    "    \n",
    "    if labeled:\n",
    "        return tweets, np.asarray(labels)\n",
    "    else:\n",
    "        return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pad_tweets(dataset, max_sequence=None):\n",
    "    \"\"\"\n",
    "    Pad each tweet to match the longest tweet present in the dataset. \n",
    "    Returns the padded dataset\n",
    "    \"\"\"\n",
    "    if not max_sequence:\n",
    "        max_sequence = max([len(tweet) for tweet in dataset])\n",
    "    padded_dataset = []\n",
    "    for tweet in dataset:\n",
    "        len_padding = max_sequence - len(tweet)\n",
    "        padded_dataset.append(tweet + ['<pad>'] * len_padding)\n",
    "\n",
    "    # TODO: Move this to the unittest\n",
    "    assert all([len(tweet) for tweet in padded_dataset]) is True\n",
    "    assert max([len(tweet) for tweet in padded_dataset]) is max_sequence\n",
    "    assert len(padded_dataset) == len(dataset)\n",
    "    \n",
    "    return padded_dataset, max_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 51\n"
     ]
    }
   ],
   "source": [
    "# Load and tokenize the dataset\n",
    "x_train, y_train = load_file(TRAIN_FILE, True)\n",
    "x_test, y_test = load_file(TEST_FILE, True)\n",
    "\n",
    "# Pad the datasets\n",
    "x_train_padded, max_sequence = pad_tweets(x_train)\n",
    "x_test_padded, _ = pad_tweets(x_test, max_sequence)\n",
    "print(len(x_train_padded[0]), len(x_test_padded[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_vocab(dataset):\n",
    "    \"\"\"\n",
    "    Given a list of lists of words, create a lookup table and a list with the vocabulary.\n",
    "    \"\"\"\n",
    "    # Count how many times a word appear in the dataset\n",
    "    word_counts = Counter(chain(*dataset))\n",
    "    # Create a list with the most common words sorted. \n",
    "    # The position will be the index of the lookup table.\n",
    "    vocab_sorted = [word for word, freq in word_counts.most_common()]\n",
    "    vocab_sorted.append('<oov>')\n",
    "    # Create a lookup table using a dictionary. Map each index with a word\n",
    "    lookup = {word: index for index, word in enumerate(vocab_sorted)}\n",
    "\n",
    "    # TODO: Move this to the unittest\n",
    "    assert len(lookup.keys()) == len(vocab_sorted)\n",
    "    \n",
    "    return vocab_sorted, lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_sorted, lookup_table = build_vocab(x_train_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_input(tweets, lookup_table):\n",
    "    # Map each word with its index from the lookup table. \n",
    "    # If the word wasn't seen during training assign the OOV token index.\n",
    "    indexes_dataset = []\n",
    "    for tweet in tweets:\n",
    "        index_tweet = [] \n",
    "        for word in tweet:\n",
    "            if word not in lookup_table.keys():\n",
    "                word = '<oov>'\n",
    "            index_tweet.append(lookup_table[word])\n",
    "        # TODO: Move this to the unittest\n",
    "        assert len(index_tweet) == len(tweet)\n",
    "        indexes_dataset.append(index_tweet)\n",
    "    # TODO: Move this to the unittest\n",
    "    assert len(indexes_dataset) == len(tweets)\n",
    "    \n",
    "    return np.asarray(indexes_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(train_filepath, test_filepath):\n",
    "    # Load and tokenize the dataset\n",
    "    x_train, y_train = load_file(train_filepath, True, True)\n",
    "    x_test, y_test = load_file(test_filepath, True, True)\n",
    "\n",
    "    # Pad the datasets\n",
    "    x_train_padded, max_sequence = pad_tweets(x_train)\n",
    "    x_test_padded, _ = pad_tweets(x_test, max_sequence)\n",
    "    \n",
    "    # Build the lookup table and the vocabulary\n",
    "    vocab_sorted, lookup_table = build_vocab(x_train_padded)\n",
    "    \n",
    "    # Create the matrices of indexes to train the system\n",
    "    x_train_indexes = create_input(x_train_padded, lookup_table)\n",
    "    x_test_indexes = create_input(x_test_padded, lookup_table)\n",
    "\n",
    "    assert x_train_indexes.shape[0] == y_train.shape[0]\n",
    "    assert x_train_indexes.shape[1] == x_test_indexes.shape[1]\n",
    "    assert x_test_indexes.shape[0] == y_test.shape[0]\n",
    "    assert y_train.shape[1] == y_test.shape[1]\n",
    "    return x_train_indexes, y_train, x_test_indexes, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_dataset(TRAIN_FILE, TEST_FILE) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
